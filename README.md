# ğŸ§¬ AI Human Detector & Explainable AI

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-FF4B4B)
![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Transformers-yellow)
![Status](https://img.shields.io/badge/Status-Live-success)

> **Ett Computer Vision-system som inte bara detekterar mÃ¤nniskor, utan ocksÃ¥ genererar en text-motivering (Captioning) fÃ¶r sina beslut. Byggt som en "Proof of Concept" fÃ¶r medicinsk bildanalys.**

---

## ğŸ–¼ï¸ Demo & Resultat

*(HÃ¤r kan du lÃ¤gga in en skÃ¤rmdump pÃ¥ nÃ¤r programmet har hittat en mÃ¤nniska och skrivit en text)*
![Demo Screenshot](https://via.placeholder.com/800x400.png?text=Ladda+upp+en+screenshot+hÃ¤r)

---

## ğŸ¥ VarfÃ¶r detta projekt? (Koppling till RÃ¶ntgen/MedTech)

Mitt mÃ¥l Ã¤r att arbeta med **AI inom medicinsk bilddiagnostik (RÃ¶ntgen/MR)**. Detta projekt Ã¤r byggt fÃ¶r att demonstrera de tvÃ¥ fundamentala tekniker som anvÃ¤nds fÃ¶r att automatisera lÃ¤karutlÃ¥tanden:

1.  **Object Detection (Var Ã¤r det?):**
    * *I detta projekt:* Ritar en grÃ¶n box runt mÃ¤nniskor.
    * *I vÃ¥rden:* AnvÃ¤nds fÃ¶r att markera frakturer, tumÃ¶rer eller organ.
2.  **Image Captioning / XAI (Vad Ã¤r det?):**
    * *I detta projekt:* Genererar en textbeskrivning (t.ex. "a woman sitting on a chair").
    * *I vÃ¥rden:* Motsvarar den automatiska rapporten (t.ex. "FÃ¶rtÃ¤tning i vÃ¤nster lunglob, misstÃ¤nkt pneumoni").

Genom att kombinera dessa tvÃ¥ modeller (DETR + BLIP) visar jag hur man bygger ett system som Ã¤r **Explainable (XAI)** â€“ nÃ¥got som Ã¤r ett krav fÃ¶r patientsÃ¤kerhet.

---

## âš™ï¸ SÃ¥ fungerar det (Under huven)

Applikationen anvÃ¤nder **Transfer Learning** med tvÃ¥ state-of-the-art modeller frÃ¥n Hugging Face:

### 1. Detektorn (Facebook DETR)
Modellen **DE**tection **TR**ansformer (ResNet-50) skannar bilden.
* Om den hittar en person med >70% sÃ¤kerhet, ritas en grÃ¶n bounding box.
* Om den hittar djur eller objekt, markeras dessa rÃ¶tt.

### 2. FÃ¶rklaringsmodellen (Salesforce BLIP)
Modellen **B**ootstrapping **L**anguage-**I**mage **P**retraining lÃ¤ser in bilden och genererar en mening som beskriver innehÃ¥llet.
* Detta ger oss "Motiveringen". Om detektorn sÃ¤ger "MÃ¤nniska", men BLIP sÃ¤ger "a statue of a man", kan vi dra slutsatsen att det inte Ã¤r en levande person.

---

## ğŸ› ï¸ Teknisk Stack

* **Frontend:** Streamlit (Python)
* **AI/ML:** PyTorch, Transformers (Hugging Face)
* **Bildhantering:** Pillow (PIL), NumPy
* **Modeller:**
    * `facebook/detr-resnet-50` (Object Detection)
    * `Salesforce/blip-image-captioning-base` (Image Captioning)

---

## ğŸš€ Hur du kÃ¶r projektet lokalt

1. **Starta appen:**
    ```bash
    streamlit run app.py
    ```

---

## ğŸ“Š Framtida fÃ¶rbÃ¤ttringar (Roadmap)

FÃ¶r att ta detta nÃ¤rmare en skarp medicinsk applikation planerar jag att:
* [ ] Finjustera (Fine-tune) modellen pÃ¥ ett dataset med rÃ¶ntgenbilder (t.ex. ChestX-ray8).
* [ ] LÃ¤gga till stÃ¶d fÃ¶r DICOM-filer (standardformatet fÃ¶r rÃ¶ntgen).
* [ ] Implementera en "Heatmap" (Grad-CAM) fÃ¶r att visuellt visa exakt vilka pixlar AI:n tittar pÃ¥.

---

## ğŸ‘¤ Kontakt

Utvecklad av **[Sabirin Matan]** â€“ Aspiring AI Engineer inom MedTech.
# HumanDetector
